---
title: "Coursera - Practical Machine Learning Project"
author: "Kelvin Leung"
date: "7/22/2018"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

### Load libraries


```{r caret_lib}
library(caret)
```
```{r rattle_lib}
library(rattle)
```
```{r party_lib}
library(party)
```

```{r rpart_lib}
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(cvTools)
```

```{r rf_lib}
library(randomForest)
```

## Data loading
```{r training}
train_data <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"), header=TRUE, na.strings = c("NA","#DIV/0!",""))
dim(train_data)
```

```{r testing}
test_data <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"), header=TRUE, na.strings = c("NA","#DIV/0!",""))
dim(test_data)
```

The training set has 19622 observations and each observation has 160 columns.
We notice that many columns have N/A values or blank values. So we will remove them because they will not produce any information. Also, the first seven columns give information about the people who did the test and the timestamps. We will remove these columns in our model. Note, the "classe" variable is in the last column of our training set. 

The testing set has 20 cases. It will be used to test the accuracy of our models. 

## Cleansing procedure
Here is the R code to remove the columns that has N/A or "" values.

```{r train_data_cleaning}
# removing columns having value of "N/A" or "" value that have at least 90% of the total number of rows in training set
tidx_2remove <- which(colSums(is.na(train_data) | train_data=='') > 0.9* dim(train_data)[1])
# removing those columns
train_clean <- train_data[ ,-tidx_2remove]
# removing the first 7 columns that are irrelevant to the prediction model
train_clean <- train_clean[ ,-(1:7)]
dim(train_clean)
str(train_clean)
```

```{r test_data_cleaning }
# removing columns having value of "N/A" or "" value that have at least 90% of the total number of rows in the test set
tidx_2remove <- which(colSums(is.na(test_data) | test_data=='') > 0.9* dim(test_data)[1])
# removing those columns
test_clean <- test_data[ ,-tidx_2remove]
# removing the first 7 columns that are irrelevant to the prediction model
test_clean <- test_clean[,-(1:7)]
dim(test_clean)
str(test_clean)
```


From the above, the columns of "train_clean" match with the columns of "test_clean", except for the last column, "problem_id". But we do not need to care about the "problem_id" column at this time.

## Build Classification models
We use "recursive partiaion tree (rpart)", "random forest (randomForest)", and "Stochastic Gradient Boosting (gbm)" to build classification models and compare their performance.

First, we partiion the training data into 2 parts and use cross validation method to validate the models we build.

To ensure the reproductivity of this experiment, we initial the seed to 12345.

```{r data_partition}
set.seed(12345)
dim(train_clean)
dim(test_clean)
tr1 <- createDataPartition(train_clean$classe, p=0.6, list=FALSE)
training <- train_clean[tr1,]
testing <- train_clean[-tr1,]
```

### Train with recursive partiaion tree (rpart)
```{r dtree}
# copy from the manual page of trainControl
seeds <- vector(mode = "list", length = 51)
for(i in 1:50) seeds[[i]] <- sample.int(1000, 22)
seeds[[51]] <- sample.int(1000, 1)

# we set the seed to 12345 to generate rpart model with 2 fold validation
set.seed(12345)
ctrl2 <- trainControl(allowParallel=T, seeds = seeds, method="cv", number=2)
mDT2 <- train(classe~., data=training, method="rpart", model=TRUE, trControl=ctrl2)

# using the same seed to generate another rpart model with 10 fold validation 
set.seed(12345)
ctrl10 <- trainControl(allowParallel=T, seeds= seeds, method="cv", number=10)
mDT10 <- train(classe~., data=training, method="rpart", model=TRUE, trControl=ctrl10)

```
## Confusion matrix using out-of-sample data in the rpart tree model
### We use "predict" to obtain out-of-sample predictions.
```{r dtree_predict}
rtree_pred_x2 <- predict(mDT2, newdata=testing)
rtree_pred_x10 <- predict(mDT10, newdata=testing)

cm_rtree_pred_x2 <- confusionMatrix(rtree_pred_x2, testing$classe)
cm_rtree_pred_x10 <- confusionMatrix(rtree_pred_x10, testing$classe)
```

### Showing the confusion matrix:
# rpart model with 2-fold validation
```{r dtree_cm_x2}
cm_rtree_pred_x2
acc_cm_rtree_X2 <- round(cm_rtree_pred_x2$overall[1], 4)

```

# rpart model with 10-fold validation
```{r dtree_cm_x10}
cm_rtree_pred_x10
acc_cm_rtree_X10 <- round(cm_rtree_pred_x10$overall[1], 4)
```

# Out-of-sample accuracy:
```{r dtree_cm_accuracy}
print.noquote(paste("Accurracy of rtree for CV n=2: ", acc_cm_rtree_X2))
print.noquote(paste("Accurracy of rtree for CV n=10: ", acc_cm_rtree_X10))
```

From the results shown above, the accuracy of rpart using cross-validation with n=10 is `r acc_cm_rtree_X10` or `r acc_cm_rtree_X10*100`%, which is more accurate than that with n=2 (i.e. `r acc_cm_rtree_X2` or `r acc_cm_rtree_X2*100`%). 

Also, the confusion matrix for rpart with 10-fold cross validation clearly shows that it has less confusion (i.e. spread away from a diagonal matrix or an identity matrix if we normalize the entires with the total number of entries) than that of rpart with 2-fold cross validation. Hence, 10-fold cross validation provides much better estimation performance than 2-fold cross validation model.

### Plot of Decison Trees
```{r decision_tree_n_2}
fancyRpartPlot(mDT2$finalModel)
```

```{r decision_tree_n_10}
fancyRpartPlot(mDT10$finalModel)
```

## Random Forest
### Training Random forest with 100 trees.
We generate a random forest model using 100 classifcation trees.

```{r rf_tree}
set.seed(12345)
mRF <- randomForest(classe ~., data=training, ntree=100, importance = TRUE)
```
I don't think it is neccessary to apply cross validation (or k-fold) in random forest becuase the performance of having out-of-bag in random forest is very simiar to cross validation. [ see https://stats.stackexchange.com/questions/283760/is-cross-validation-unnecessary-for-random-forest ].


### Plotting the out-of-sample error of the random forest vs. num. of trees
```{r rf_plot}
plot(mRF, main='Plot of out-of-sample error for random forest vs. num. of trees')
legend("right", colnames(mRF$err.rate), col=1:ncol(test_clean), cex=0.8, fill=1:ncol(test_data))
```


## Plotting the important variables for the classification problem. 
```{r varImp}
varImpPlot(mRF)
```

From the plots above, it shows that "mean descrease accuracy" and "mean decrease gini". 


### Confusion matrix using out-of-sample data in the random forest model
```{r rf_predict}
pred_rf <- predict(mRF, newdata=testing, type='class')
cm_rf <- confusionMatrix(pred_rf, testing$classe)
cm_rf
```

Notice that the confusion matrix of random forest model is less confused than that of rpart model above. 


## Building the Boosting Model
```{r boosting}
set.seed(12345)
mbst <- train(classe ~., method="gbm", data=training, verbose=F, trControl=trainControl(method="cv", number=10))
mbst
```

```{r plotBoost}
plot(mbst)
```

### Out-of-sample using confusion matrix
```{r predict_Boost}
pred_bt <- predict(mbst, newdata=testing)
confusionMatrix(pred_bt, testing$classe)
```

## Classification of Uknonw test data
### Decision Tree Model
```{r dtree_unknown_test_data}
pred_dT <- predict(mDT2, newdata=test_clean)
pred_dT
```

### Random forest model
```{r rf_unknown_test}
pred_rf <- predict(mRF, newdata=test_clean)
pred_rf
```

### Stochastic Gradient Boosting (gbm)
```{r rf_unknown_test_boost}
pred_bt <- predict(mbst, newdata=test_clean)
pred_bt
```

I beleive that Random Forest model (n=100) and Stochastic Gradient Boosting (gbm) are very accurate in terms of the out-of-sample accuracy, i.e. 99.44% for Random Forest 96.39% for gbm and ~50% for recursive partiaion tree (rpart) respectively.  We can reply on either Random Forest or Stochastic Gradient Boosting (gbm) for the prediction. One caution is that the time it took to build Random Forest is much faster than that for gbm. Therefore, we would decide to use Random Forest to get the perliminary prediction result as much as possible.


